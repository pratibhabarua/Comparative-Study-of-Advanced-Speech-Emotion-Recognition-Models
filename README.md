# Comparative Study of Advanced Speech Emotion Recognition Models

This repository contains the implementation and evaluation of various advanced speech emotion recognition models. The models studied include CNN, RNN, LSTM, ANN, and hybrid architectures like RCNN and LSTM+CNN. The goal is to compare their performance using acoustic features like MFCCs, pitch, and energy.

## Models Evaluated

- **CNN (Convolutional Neural Networks)**
- **RNN (Recurrent Neural Networks)**
- **LSTM (Long Short-Term Memory Networks)**
- **ANN (Artificial Neural Networks)**
- **Hybrid Models**
  - RCNN (Recurrent Convolutional Neural Networks)
  - LSTM+CNN

## Acoustic Features

The following acoustic features are utilized in the models:
- **MFCCs (Mel-Frequency Cepstral Coefficients)**
- **Pitch**
- **Energy**

## Model Capabilities

- **CNNs:** Capture spatial hierarchies in the data.
- **RNNs:** Handle temporal dependencies.
- **LSTMs:** Manage long-term dependencies effectively.
- **ANNs:** Serve as baseline models for comparison.

## Hybrid Models

Combining CNN and RNN/LSTM architectures aims to leverage the strengths of both types of networks for improved performance.

## Summary

This study explores the potential of different deep learning architectures for the task of speech emotion recognition, focusing on their ability to utilize acoustic features and manage various dependencies within the data. The results from these models provide insights into their comparative effectiveness in recognizing emotions from speech.
